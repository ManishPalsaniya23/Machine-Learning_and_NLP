I build a next word predictor using a transformer architecture based model GPT-2 available on Hugging Face's transformers library.

Fine tuned GPT-2 on textual dataset of **WikiText-2** Corpus which is a large text corpus containing clean Wikipedia articles which are suitable for structured language learning.

**Trained the model using the Trainer API provided by  Hugging face's transformers library**. 

