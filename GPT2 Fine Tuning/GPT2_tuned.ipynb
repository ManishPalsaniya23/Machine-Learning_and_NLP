{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers tokenizers datasets evaluate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean broken cache\n",
    "!rm -rf ~/.cache/huggingface/datasets/wikitext\n",
    "!rm -rf /content/hf_cache\n",
    "\n",
    "#Downgrade fsspec to avoid glob error\n",
    "!pip install -U \"fsspec<2023.9.0\" datasets transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", cache_dir=\"/content/hf_cache\")\n",
    "\n",
    "def clean_and_tokenize(example):\n",
    "    if example[\"text\"].strip() == \"\":\n",
    "        return {\"input_ids\": [], \"attention_mask\": []}\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(clean_and_tokenize, batched=False, remove_columns=[\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda x: len(x[\"input_ids\"]) > 0)\n",
    "\n",
    "# testing tokenizer\n",
    "print(tokenized_datasets[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "# grouping the data into chunks of fixed length for training and model intitialization\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated[\"input_ids\"])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_datasets.map(group_texts, batched=True)\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-wikitext2\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    fp16= True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating perplexity and top k accuracy\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "perplexity = load(\"perplexity\")\n",
    "results = perplexity.compute(model_id='gpt2', predictions=[\"The capital of France is\"])\n",
    "print(\"Perplexity:\", results)\n",
    "\n",
    "def compute_top_k_accuracy(logits, labels, k=5):\n",
    "    _, top_k_preds = torch.topk(logits, k, dim=-1)\n",
    "    correct = top_k_preds.eq(labels.unsqueeze(-1)).any(-1).float()\n",
    "    return correct.mean().item()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get the device of the model\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "sample = lm_dataset[\"validation\"][0]\n",
    "inputs = torch.tensor([sample[\"input_ids\"]]).to(device) # Move inputs to the same device as the model\n",
    "labels = torch.tensor([sample[\"labels\"]]).to(device) # Move labels to the same device as the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits[:, :-1, :]\n",
    "    labels = labels[:, 1:]\n",
    "    top_k_acc = compute_top_k_accuracy(logits[0], labels[0])\n",
    "    print(f\"Top-K Accuracy: { top_k_acc*100 :.3f}% \")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model(\"./gpt2-wikitext2-final\")\n",
    "tokenizer.save_pretrained(\"./gpt2-wikitext2-final\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloading model to test\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-wikitext2-final\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Reloading model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./gpt2-wikitext2-final\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-wikitext2-final\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"The theory of relativity states that\"\n",
    "outputs = generator(prompt, max_new_tokens=100, do_sample=True, top_k=20)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])\n",
    "\n",
    "prompts = [\n",
    "\n",
    "    \"The theory of relativity states that\",\n",
    "    \"Quantum computers are expected to\",\n",
    "    \"Artificial intelligence can help in\",\n",
    "    \"The capital of India is\"\n",
    "]\n",
    " # with temp, which allow weights to tokens with low prob\n",
    "for prompt in prompts:\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"GPT-2 :\", generator(prompt, max_new_tokens=50, do_sample=True, top_k=40, temperature=0.8))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}